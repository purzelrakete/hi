# TODO

- implement gradient descent for binary logreg
- return and graph gradient descent debugging data
- get one vs all modelled cleanly
- train correctly for each class
- optimize speed. currently not usable
- evaluate binary logreg with one vs all
- tidy code:
  * remove assumption that classes are 0:n
- introduce bias terms for binary logreg
- evaluate binary logreg with one vs all + bias
- introduce regularization for binary logreg
- evaluate binary logreg with one vs all + bias + regularization
- do a MLE linear model, also with regularization
- evaluate MLE linear model, also with regularization
- understand the GLM framework, why sigmoid is natural
- do softmax multinomial logreg with full working
- switch to SGD
- switch to minibatch
